\section{Our problem}

Part of our problem is the detection and classification of faults in the HVAC system of the SE2 building in the Merced campus of the University of California. Such system has an approximate of 2000 sensor and we take samples each 5 minutes. It is our intention to use data from the past 2 years to train our model, this gives us a size for the training set of $n = 210240$ training samples. Thus, lets assume that our ANN will have the following topology: $m = 2000$ for the 2000 input features (this is just a rough estimate since we will apply dimensionality reduction and feature extraction techniques to reduce this number as much as we can), $k = 4$ layers for an ANN of 1 input layer, 2 hidden layers and 1 output layer. In this case we assume that the number of output layers (fault classes) $o$ is much smaller than the number of input layers, we expect a number on the order of $100$ different types of faults, as for the number of neurons on each hidden layer we will set it arbitrary to $2000$ (the number of input neurons) as it has been shown that the number of hidden layers for other classification/regression problems is usually smaller than the number of input layers, thus, setting the number of hidden layers equal to the number of input layers will help us set a boundary on the number of computations.

Using the above information and eq. \eqref{eq:ann_complexity1} the asymptotic complexity of our ANN will be $O(210240 * 2000 * 2000^2 * 100) = O(1.6819 \times 10^{17})$ FLOPS, this neglecting the computational time spent on the minimization problem inherent to the computation of the weights for the ANN.

\section{Conclusions}

Based on the estimate of FLOPS our equipment can handle which within the order of $3 \times 10^{19}$ FLOPS/month and the complexity for the training of our modeled ANN we estimate our which is of the order of $O(1.6819 \times 10^{17}$ FLOPS we conclude our system can do the training for our model in approximately one month of computations, provided that we take the most of our equipment, such as the pipe-lining and parallel computing.